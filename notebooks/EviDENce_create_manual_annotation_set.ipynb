{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select folder with text fragments in the zip folder on surfdrive:**\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize100/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*NB: Make sure to extract the zip folder on high-level location on your computer a to avoid \"path-too-long\" error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide path to extracted folder with lemma fragments\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"lem_par_150\",\"lemma_preserve_paragraph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_search import create_searchable_data\n",
    "\n",
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "#create_searchable_data(search_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import open_dir\n",
    "\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "ix = open_dir(indexdir)\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    index_dic = {doc['title']:[doc['textdata']] for doc in searcher.all_stored_fields()}   \n",
    "\n",
    "# Store document information in pandas dataframe\n",
    "index_df = pd.DataFrame.from_dict(index_dic, orient='index')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataframe in csv\n",
    "manual_annotation_set = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"manual_annotation_set.csv\")\n",
    "index_df.to_csv(manual_annotation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

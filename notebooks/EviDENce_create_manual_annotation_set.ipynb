{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data2\n",
    "\n",
    "# Define paths\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"TargetSize150\",\"text_preserve_paragraph\")\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select folder with text fragments in the zip folder on surfdrive:**\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize100/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*NB: The file names are long, and so is the path. Make sure to extract the zip folder on high-level location on your computer to avoid \"path-too-long\" error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "create_searchable_data2(search_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Store required data for manual annotation **\n",
    "\n",
    "* Store data of all fragments in corpus in a dataframe\n",
    "* Take a random sample\n",
    "* Store the sample, i.e., title+text, in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = open_dir(indexdir)\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    index_dic = {doc['title']:[doc['textdata']] for doc in searcher.all_stored_fields()}   \n",
    "\n",
    "index_df = pd.DataFrame.from_dict(index_dic, orient='index')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1111\n",
    "sample = index_df.sample(n=100, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store file set used for manual annotation**\n",
    "\n",
    "* Based on the sample created above, i.e., csv file with for every fragment title+text\n",
    "* Store for every textfragment _mentioned_ in the csv file the corresponding _actual_ lemma-based fragment in a separate folder\n",
    "* The automatic analyses, i.e., keyword search and machine learning, can be performed on the fragments in that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import re\n",
    "\n",
    "sample_file = \"manual_annotation_sample_seed1111.csv\"\n",
    "manual_annotation_sample = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Data\",sample_file)\n",
    "\n",
    "# Collect titles in sample from stored csv file NB these are files with text and not lemmas\n",
    "mannotate_df = pd.read_csv(manual_annotation_sample,sep=',',encoding = \"ISO-8859-1\")\n",
    "\n",
    "source = os.path.join(os.sep,root,\"TargetSize150\",\"lemma_preserve_paragraph\")\n",
    "destination = os.path.join(os.sep,root,\"TargetSize150\",\"mannotate_lemma_sample1111\")\n",
    "delimiter = '_clipped'\n",
    "\n",
    "for title in mannotate_df['Titel'].iloc[:-1].tolist():\n",
    "    # Recreate folder name from file title\n",
    "    fol,rest = title.split(delimiter)\n",
    "    fol = fol + delimiter\n",
    "    # Adapt title to point to lemma -and not text-file\n",
    "    file = title.replace(\"text\",\"lemma.txt\")\n",
    "    srcpath = os.path.join(os.sep,source,fol,file)\n",
    "    destpath = os.path.join(os.sep,destination)\n",
    "    # Copy file to destination folder\n",
    "    shutil.copy(srcpath, destpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

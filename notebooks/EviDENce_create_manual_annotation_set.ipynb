{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data2\n",
    "from baseline_search import copy_fragments\n",
    "\n",
    "# Define paths\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"TargetSize150\",\"text_preserve_paragraph\")\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "manual_path = os.path.join(root,\"TargetSize150\",\"Manual_annotation_sets-20190110T093859Z-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select folder with text fragments in the zip folder on surfdrive:**\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize100/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*NB: The file names are long, and so is the path. Make sure to extract the zip folder on high-level location on your computer to avoid \"path-too-long\" error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opening the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "#create_searchable_data2(search_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Store required data for manual annotation **\n",
    "\n",
    "* Store data of all fragments in corpus in a dataframe\n",
    "* Take a random sample\n",
    "* Store the sample, i.e., title+text, in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = open_dir(indexdir)\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    index_dic = {doc['title']:[doc['textdata']] for doc in searcher.all_stored_fields()}   \n",
    "\n",
    "index_df = pd.DataFrame.from_dict(index_dic, orient='index')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1111\n",
    "sample = index_df.sample(n=100, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store file set used for manual annotation**\n",
    "\n",
    "* Based on the sample created above, i.e., csv file with for every fragment title+text\n",
    "* Store for every textfragment _mentioned_ in the csv file the corresponding _actual_ lemma-based fragment in a separate folder\n",
    "* The automatic analyses, i.e., keyword search and machine learning, can be performed on the fragments in that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = os.path.join(os.sep,root,\"TargetSize150\",\"lemma_preserve_paragraph\")\n",
    "dest = os.path.join(manual_path,\"Auto_annotation_sets\")\n",
    "\n",
    "for item in os.listdir(manual_path):\n",
    "    path = os.path.join(manual_path,item)\n",
    "    if os.path.isfile(path):\n",
    "        copy_fragments(path,source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "\n",
    "for item in os.listdir(manual_path):\n",
    "    path = os.path.join(manual_path,item)\n",
    "    if os.path.isfile(path):\n",
    "        file_df = pd.read_csv(path, sep=',', encoding = \"ISO-8859-1\").dropna(how='all')\n",
    "        titles = file_df['Titel'].iloc[:-1].tolist()\n",
    "        all_titles.append(titles)\n",
    "\n",
    "manual_set = [title for sublist in all_titles for title in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

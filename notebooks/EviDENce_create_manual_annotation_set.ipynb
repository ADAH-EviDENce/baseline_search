{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data\n",
    "\n",
    "# Define paths\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"TargetSize150\",\"text_preserve_paragraph\")\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select folder with text fragments in the zip folder on surfdrive:**\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize100/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*NB: The file names are long, and so is the path. Make sure to extract the zip folder on high-level location on your computer to avoid \"path-too-long\" error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "create_searchable_data(search_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Store required data for manual annotation **\n",
    "\n",
    "* First store in dataframe\n",
    "* Create random sample\n",
    "* Store sample in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = open_dir(indexdir)\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    index_dic = {doc['title']:[doc['textdata']] for doc in searcher.all_stored_fields()}   \n",
    "\n",
    "# Store document information in pandas dataframe\n",
    "index_df = pd.DataFrame.from_dict(index_dic, orient='index')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1111\n",
    "sample = index_df.sample(n=100, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataframe in csv together with seed value for reproduci\n",
    "sample_file = \"manual_annotation_sample_seed{}.csv\".format(random_state)\n",
    "\n",
    "manual_annotation_sample = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Data\",sample_file)\n",
    "sample.to_csv(manual_annotation_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

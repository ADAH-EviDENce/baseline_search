{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline search engine for EviDENce\n",
    "\n",
    "Search strategy\n",
    "\n",
    "1. Collect corpus to perform search on\n",
    "2. Index documents in corpus\n",
    "3. Collect Keywords\n",
    "4. Construct query\n",
    "5. Perform search\n",
    "6. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data\n",
    "from baseline_search import create_lemma\n",
    "from baseline_search import eng_to_dutch\n",
    "from baseline_search import quote_phrases\n",
    "\n",
    "# Define paths:\n",
    "\n",
    "# Path to extracted folder with lemma fragments\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"TargetSize150\",\"lemma_preserve_paragraph\")\n",
    "# Path to alternative folder with lemma fragments\n",
    "surf = os.path.join(os.sep,root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Data\",\"NR-Teksts\",\"EviDENce_NR_output\")\n",
    "alt_search_dir = os.path.join(os.sep,surf,\"Size200\",\"fragmented_lemmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Collect corpus to perform search on**\n",
    "\n",
    "Our corpus consists of oral history accounts.\n",
    "These are broken up in text fragments -of 150 lemmas each- and can be found in a zip folder on surfdrive:\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize150/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*Both the file names and path names are long. Make sure to extract the zip folder on high-level location on your computer a to avoid \"path-too-long\" error*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "#create_searchable_data(search_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Collect list of keywords from CEO-ECB mappings**\n",
    "\n",
    "Keywords are based on mappings from classes of the Circumstantial Event Ontology (CEO) on the ECB+ corpus\n",
    "\n",
    "Preprocessing entails:\n",
    "* manually select keywords related to violence\n",
    "* express keywords as lemmas to ensure more effective matching \n",
    "* translate selected keywords to Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_file =\"../data/MdV_selectedCEOECB.csv\"\n",
    "mention_df = pd.read_csv(mention_file,sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "mention_df['wordnet_lemma']=mention_df.apply(lambda x:create_lemma(x['Mention']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As automatic translation is not stable or does not provide sufficient results, please use following workaround**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_file = \"../data/Translated_lemmas.csv\"\n",
    "translation_df = pd.read_csv(translation_file,sep=',',encoding = \"ISO-8859-1\") \n",
    "\n",
    "mention_df['Dutch']= translation_df['Dutch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_mention_df= pd.DataFrame(data =mention_df['CEO class'])\n",
    "ceo_mention_df = ceo_mention_df.set_index(mention_df['Dutch'])\n",
    "ceo_mention_dict = ceo_mention_df.to_dict(orient ='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Construct query**\n",
    "\n",
    "* Sort keywords\n",
    "* Add double quotes to phrase queries\n",
    "* Concatenate all keywords into one query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_mention_list = list(set(nl_mentions))\n",
    "quoted_nl_mention_list = quote_phrases (nl_mention_list)\n",
    "nl_mention_query = \",\".join(quoted_nl_mention_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Perform search**\n",
    "\n",
    "Using whoosh library:\n",
    "\n",
    "* Define query parser: which schema, which search fields, AND/OR search\n",
    "* Define searcher: which scoring approach\n",
    "* Store info from results object in pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from whoosh import scoring\n",
    "from whoosh import qparser\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "ix = open_dir(indexdir)\n",
    "\n",
    "parser = qparser.QueryParser(\"content\", schema=ix.schema,group=qparser.OrGroup)\n",
    "my_query = parser.parse(nl_mention_query)\n",
    "\n",
    "cols_list = []\n",
    "titles_list = []\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    results = searcher.search(my_query,limit=None, terms = True)\n",
    "    for res in results:\n",
    "        titles_list.append(res[\"title\"])\n",
    "        #row_dict = {}\n",
    "        col_dict = defaultdict(int)\n",
    "        hits = [term.decode('utf8')  for where,term in res.matched_terms()]\n",
    "        for hit in hits:\n",
    "            col_dict[hit]+= 1\n",
    "            #row_dict[hit] = row_dict.get(hit, 0) + 1  \n",
    "        cols_list.append(col_dict)\n",
    "\n",
    "#Create a dataframe for results of this search, i.e. with a limited set of keywords \n",
    "results_df = pd.DataFrame(cols_list)\n",
    "results_df.set_index([titles_list], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ix.searcher() as searcher:\n",
    "    index_dic = {doc['title']:[doc['textdata']] for doc in searcher.all_stored_fields()}   \n",
    "\n",
    "index_df = pd.DataFrame.from_dict(index_dic, orient='index')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search results dataframe contains:\n",
    "* only those keywords that are found in documents\n",
    "* only those documents that have one or more keywords\n",
    "\n",
    "Combined dataframe contains: \n",
    "* all keywords, also those that are not present in documents\n",
    "* all documents, also those that have no keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for all docs and keywords with empty values\n",
    "keywords_dic = {term:0 for term in quoted_nl_mention_list}\n",
    "list_docs = [doc['title'] for doc in ix.searcher().documents()] \n",
    "\n",
    "all_df = pd.DataFrame(keywords_dic, index = list_docs)\n",
    "\n",
    "# Create a dataframe for all docs and keywords with search results\n",
    "merged_df = results_df.combine_first(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently phrase queries are still broken up in separate search terms\n",
    "# this is shown by the surplus in columns in merged_df\n",
    "surplus = [col for col in merged_df if col not in all_df]\n",
    "# Remove these for now as a workaround; phrase queries should be fixed\n",
    "merged_df = merged_df.drop(columns = surplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Process and describe results** \n",
    "\n",
    "* Describe general characteristics of baseline search\n",
    "    * Original corpus size\n",
    "    * Number of keywords\n",
    "    * csv with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = ix.searcher().documents() \n",
    "summed_docs = sum(1 for x in all_docs)\n",
    "\n",
    "summed_results =len(results_df.index)\n",
    "percent_hits = (summed_results/summed_docs)*100\n",
    "\n",
    "percent_keywords = (len(results_df.columns)/len(quoted_nl_mention_list))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_summary = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Baseline summary.txt\")\n",
    "\n",
    "with open(report_summary, 'w') as file_handler:\n",
    "    # Add path to corpus\n",
    "    file_handler.write(\"Original corpus size: %s \\n\"%summed_docs)\n",
    "    file_handler.write(\"Number of snippets with keyword(s) present: %s \\n\"%summed_results)\n",
    "    file_handler.write(\"Percentage snippets with keywords(s) in corpus: %s \\n\"%percent_hits)\n",
    "    # Add path to keywords\n",
    "    file_handler.write(\"Total number of unique keywords found: %s \\n\"%len(results_df.columns))\n",
    "    file_handler.write(\"Percentage keywords found wrt to set used in query: %s \\n\"%percent_keywords)\n",
    "    file_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw data\n",
    "report_raw_data = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Baseline results.csv\")\n",
    "merged_df.to_csv(report_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze results\n",
    "    * nr keywords found per document\n",
    "    * nr keywords found per category\n",
    "    * nr hits found per keyword \n",
    "    * missed keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for documents\n",
    "sum_docs = pd.Series(merged_df.sum(axis=1)).value_counts(sort=True)\n",
    "sum_docs = sum_docs.sort_index()\n",
    "\n",
    "p1 = sum_docs.plot(kind='bar')\n",
    "\n",
    "p1.set_title('Figure 1: Distribution of keywords (total nr) in the searched corpus')\n",
    "\n",
    "p1.get_figure().savefig(os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Hits_per_document.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for keywords\n",
    "sum_keywords = pd.Series(merged_df.sum().iloc[:-1])\n",
    "\n",
    "sum_keywords = sum_keywords.sort_values(ascending=True)\n",
    "p2 = sum_keywords.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p2.set_title('Figure 2: Total nr of hits for top 35 most frequent keywords')\n",
    "\n",
    "p2.get_figure().savefig(os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Freq_keywords.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store list of keywords that were not present in analyzed corpus\n",
    "report_missed_keywords = os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Keywords not found in corpus.csv\")\n",
    "\n",
    "missed_keywords = sum_keywords[sum_keywords == 0]\n",
    "missed_keywords.to_csv(report_missed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

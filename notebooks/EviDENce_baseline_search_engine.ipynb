{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline search engine for EviDENce\n",
    "\n",
    "Search strategy\n",
    "\n",
    "1. Collect corpus to perform search on\n",
    "2. Index documents in corpus\n",
    "3. Collect Keywords\n",
    "4. Perform search\n",
    "6. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data\n",
    "from baseline_search import create_searchable_data2\n",
    "from baseline_search import create_lemma\n",
    "from baseline_search import eng_to_dutch\n",
    "from baseline_search import search_corpus\n",
    "from baseline_search import quote_phrase\n",
    "\n",
    "# disable SettingWithCopyWarning warning (default='warn')\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths:\n",
    "# to extracted folder with lemma fragments\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "manual_set = os.path.join(root,\"TargetSize150\",\"Manual_annotation_sets-20190110T093859Z-001\")\n",
    "search_dir = os.path.join(manual_set,\"Auto_annotation_sets\")\n",
    "# to reports on baseline_search results\n",
    "report = os.path.join(manual_set,\"Reports\")\n",
    "# to ground truth\n",
    "gt_file = os.path.join(manual_set,\"Manual Annotation Jeroen en Susan\",\"Manual annotation TOTAAL door Jeroen en Susan.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect corpus to perform search on\n",
    "\n",
    "Our corpus consists of oral history accounts.\n",
    "These are broken up in text fragments -of 150 lemmas each- and can be found in a zip folder on surfdrive:\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize150/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*Both the file names and path names are long. Make sure to extract the zip folder on high-level location on your computer a to avoid \"path-too-long\" error*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "#create_searchable_data(search_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Collect list of keywords from an existing vocabulary\n",
    "\n",
    "Preprocessing entails:\n",
    "* manually select keywords related to violence\n",
    "* express keywords as lemmas to ensure more effective matching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.  Keywords from CEO-ECB mappings ####\n",
    "\n",
    "Keywords based on mappings from classes of the Circumstantial Event Ontology (CEO) on the ECB+ corpus\n",
    "* translate selected keywords to Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_file =\"../data/MdV_selectedCEOECB.csv\"\n",
    "ceo_df = pd.read_csv(ceo_file,sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "ceo_df['wordnet_lemma']=ceo_df.apply(lambda x:create_lemma(x['Mention']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: As automatic translation is not stable or does not provide sufficient results, please use following workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transl_file = \"../data/Translated_lemmas.csv\"\n",
    "transl_df = pd.read_csv(transl_file,sep=',',encoding = \"ISO-8859-1\") \n",
    "\n",
    "ceo_df['Dutch']= transl_df['Dutch']\n",
    "# Add quotes in case of multiple words to enable 'phrase queries'.\n",
    "ceo_df['Dutch']= ceo_df.apply(lambda x:quote_phrase(x['Dutch']), axis=1)\n",
    "ceo_list = list(ceo_df['Dutch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary with CEO class per unique keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_class_df = pd.DataFrame(data =ceo_df[['CEO class','Dutch']]).drop_duplicates(subset= 'Dutch')\n",
    "ceo_class_df = ceo_class_df.drop(columns=['Dutch']).set_index(ceo_class_df['Dutch'])\n",
    "ceo_class_dict = ceo_class_df.transpose().to_dict(orient = 'records')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.  Keywords from WW2 Thesaurus ####\n",
    "\n",
    "The WW2 Thesaurus describes keywords related to events, locations, concepts and objects from the second world war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "thes_file =\"../data/Geweldslexicon obv WOIIThesaurus SH en JW.csv\"\n",
    "thes_df = pd.read_csv(thes_file,sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual selection of relevant violence keywords is done by two annotators. \n",
    "\n",
    "Either take union or intersection of two manual selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "susan = thes_df['Susan'].dropna().str.lower().tolist()\n",
    "jeroen = thes_df['Jeroen'].dropna().str.lower().tolist()\n",
    "\n",
    "thes_intersect = [x for x in susan if x in jeroen]\n",
    "thes_union = set(susan+jeroen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform search ###\n",
    "\n",
    "Using whoosh library:\n",
    "* Define query parser: which schema, which search fields, AND/OR search\n",
    "* Define searcher: which scoring approach\n",
    "\n",
    "Store info from results object in pandas dataframe, which contain\n",
    "* all keywords, also those that are not present in documents\n",
    "* all documents, also those that have no keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "\n",
    "# Search results for selected keyword list\n",
    "merged_df = search_corpus(indexdir,ceo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with groundtruth ###\n",
    "\n",
    "1. From ground truth: determine which fragments are relevant, i.e., violent fragments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ground truth dataframe\n",
    "gt_df = pd.read_csv(gt_file,sep=',',encoding = \"ISO-8859-1\") \n",
    "\n",
    "# Random set for manual annotation contains duplicate fragments; remove these\n",
    "gt_all_df = gt_df.drop_duplicates(subset = ['Titel'])\n",
    "# Select all violent fragments\n",
    "gt_vio_df = gt_all_df[gt_all_df['JA/NEE'].isin(['ja','Ja'])]\n",
    "\n",
    "# Add columns with clean titles to enable comparison\n",
    "titles = gt_vio_df['Titel'].tolist()\n",
    "new_titles = [title.split('_text')[0] for title in titles]\n",
    "se = pd.Series(new_titles)\n",
    "gt_vio_df['clean title'] = se.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From baseline: determine which fragments are correctly marked as violent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 173)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['hits']=merged_df.sum(axis=1)\n",
    "base_vio_df = merged_df[merged_df['hits'] > 0]\n",
    "base_vio_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['hits']=merged_df.sum(axis=1)\n",
    "\n",
    "base_vio_df = merged_df[merged_df['hits'] > 0]\n",
    "\n",
    "# Add columns with clean titles to enable comparison\n",
    "titles = base_vio_df.index.values.tolist()\n",
    "new_titles = [title.split('_lemma')[0] for title in titles]\n",
    "se = pd.Series(new_titles)\n",
    "base_vio_df['clean title'] = se.values\n",
    "\n",
    "# Select correctly annotated violent fragments\n",
    "base_vio_df['correct']= base_vio_df['clean title'].isin(gt_vio_df['clean title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Determine recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5381944444444444\n",
      "Precision: 0.41778975741239893\n"
     ]
    }
   ],
   "source": [
    "relevant = len(gt_vio_df['clean title'])\n",
    "retrieved = len(base_vio_df['clean title'])\n",
    "correct_retrieved = len(base_vio_df[base_vio_df['correct'] == True])\n",
    "\n",
    "recall = correct_retrieved/relevant\n",
    "precision = correct_retrieved/retrieved\n",
    "\n",
    "print('Recall: {}'.format(recall))\n",
    "print('Precision: {}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aanval', 'arm', 'arresteren', 'bezetting', 'bom', 'bombardement',\n",
       "       'brand', 'breuk', 'conflict', 'doden', 'dood', 'doodslag',\n",
       "       'gebombardeerd', 'geslagen', 'gesloten', 'gevangen', 'gevangenis',\n",
       "       'geweer', 'geweld', 'gewond', 'hangen', 'herrie', 'inval', 'kwaad',\n",
       "       'missen', 'moord', 'onderzoek', 'oorlog', 'raken', 'roken', 'rook',\n",
       "       'schade', 'schieten', 'schot', 'staking', 'steken', 'stempel',\n",
       "       'sterven', 'straf', 'straffen', 'strijd', 'vermoorden', 'vernietigen',\n",
       "       'veroordeling', 'verraden', 'verwoesting', 'vluchten', 'wond',\n",
       "       'zelfmoord', 'hits', 'clean title', 'correct'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_report = os.path.join(report, \"base_vs_gt.csv\")\n",
    "\n",
    "base_vio_df.to_csv(compare_report)\n",
    "\n",
    "# Which keywords were succesful?\n",
    "correct_df = base_vio_df[base_vio_df['correct'] == True]\n",
    "clean_correct_df = correct_df.loc[:, (correct_df != 0).any(axis=0)]\n",
    "clean_correct_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for keywords\n",
    "succesful_keywords = pd.Series(clean_correct_df.sum().iloc[:-1])\n",
    "\n",
    "sum_keywords = sum_keywords.sort_values(ascending=True)\n",
    "p6 = sum_keywords.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p6.set_title('Figure 6: Total nr of hits for succesful keywords')\n",
    "\n",
    "p6.get_figure().savefig(os.path.join(report,\"Succesful_keywords.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Process and describe results** \n",
    "\n",
    "* Describe general characteristics of baseline search\n",
    "    * Original corpus size\n",
    "    * Number of keywords\n",
    "    * csv with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = ix.searcher().documents() \n",
    "summed_docs = sum(1 for x in all_docs)\n",
    "\n",
    "summed_results =len(results_df.index)\n",
    "percent_hits = (summed_results/summed_docs)*100\n",
    "\n",
    "percent_keywords = (len(results_df.columns)/len(nl_mention_list))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_summary = os.path.join(report,\"Baseline_summary.txt\")\n",
    "\n",
    "with open(report_summary, 'w') as file_handler:\n",
    "    # Add path to corpus\n",
    "    file_handler.write(\"Original corpus size: %s \\n\"%summed_docs)\n",
    "    file_handler.write(\"Number of snippets with keyword(s) present: %s \\n\"%summed_results)\n",
    "    file_handler.write(\"Percentage snippets with keywords(s) in corpus: %s \\n\"%percent_hits)\n",
    "    # Add path to keywords\n",
    "    file_handler.write(\"Total number of unique keywords found: %s \\n\"%len(results_df.columns))\n",
    "    file_handler.write(\"Percentage keywords found wrt to set used in query: %s \\n\"%percent_keywords)\n",
    "    file_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw data\n",
    "report_raw_data = os.path.join(report,\"Baseline_results.csv\")\n",
    "merged_df.to_csv(report_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze results\n",
    "    * nr keywords found per document\n",
    "    * nr keywords found per category\n",
    "    * nr hits found per keyword \n",
    "    * missed keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for documents\n",
    "sum_docs = pd.Series(merged_df.sum(axis=1)).value_counts(sort=True)\n",
    "sum_docs = sum_docs.sort_index()\n",
    "\n",
    "p1 = sum_docs.plot(kind='bar')\n",
    "\n",
    "p1.set_title('Figure 1: Distribution of keywords (total nr) in the searched corpus')\n",
    "\n",
    "p1.get_figure().savefig(os.path.join(report,\"Hits_per_document.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for keywords\n",
    "sum_keywords = pd.Series(merged_df.sum().iloc[:-1])\n",
    "\n",
    "sum_keywords = sum_keywords.sort_values(ascending=True)\n",
    "p2 = sum_keywords.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p2.set_title('Figure 2: Total nr of hits for top 35 most frequent keywords')\n",
    "\n",
    "p2.get_figure().savefig(os.path.join(report,\"Freq_keywords.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store list of keywords that were not present in analyzed corpus\n",
    "report_missed_keywords = os.path.join(report,\"Keywords_not_found_in_corpus.csv\")\n",
    "\n",
    "missed_keywords = sum_keywords[sum_keywords == 0]\n",
    "missed_keywords.to_csv(report_missed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ceo = pd.Series(ceo_mention_df2['CEO class']).value_counts(sort=True,ascending=True).drop(labels=['/B','/M','/P','\\P','/O'])\n",
    "\n",
    "p3 = count_ceo.plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p3.set_title('Figure 3: Distribution of classes of keywords in the query')\n",
    "\n",
    "p3.get_figure().savefig(os.path.join(report,\"Keyword_classes.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_sum_keywords = [ceo_mention_dict[i] for i in sum_keywords.index]\n",
    "\n",
    "serie = pd.Series(sum_keywords.values, index=ceo_sum_keywords)\n",
    "\n",
    "res = serie.groupby(serie.index).sum()\n",
    "res = res.drop(labels=['\\P','/M','/B']).sort_values()\n",
    "\n",
    "p4 = res.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p4.set_title('Figure 4: Distribution of classes of keywords in search results')\n",
    "\n",
    "p4.get_figure().savefig(os.path.join(report,\"Freq_keywords_classes.png\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

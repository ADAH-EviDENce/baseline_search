{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline search engine for EviDENce\n",
    "\n",
    "Search strategy\n",
    "\n",
    "1. Collect corpus to perform search on\n",
    "2. Index documents in corpus\n",
    "3. Collect Keywords\n",
    "4. Construct query\n",
    "5. Perform search\n",
    "6. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# Imports from own script\n",
    "from baseline_search import create_searchable_data\n",
    "from baseline_search import create_searchable_data2\n",
    "from baseline_search import create_lemma\n",
    "from baseline_search import eng_to_dutch\n",
    "from baseline_search import quote_phrase\n",
    "\n",
    "# Define paths:\n",
    "\n",
    "# Path to extracted folder with lemma fragments\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "manual_set = os.path.join(root,\"TargetSize150\",\"Manual_annotation_sets-20190110T093859Z-001\")\n",
    "search_dir = os.path.join(manual_set,\"Auto_annotation_sets\")\n",
    "# Path to reports on baseline_search results\n",
    "report = os.path.join(manual_set,\"Reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Collect corpus to perform search on**\n",
    "\n",
    "Our corpus consists of oral history accounts.\n",
    "These are broken up in text fragments -of 150 lemmas each- and can be found in a zip folder on surfdrive:\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize150/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*Both the file names and path names are long. Make sure to extract the zip folder on high-level location on your computer a to avoid \"path-too-long\" error*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "create_searchable_data(search_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Collect list of keywords from CEO-ECB mappings**\n",
    "\n",
    "Keywords are based on mappings from classes of the Circumstantial Event Ontology (CEO) on the ECB+ corpus\n",
    "\n",
    "Preprocessing entails:\n",
    "* manually select keywords related to violence\n",
    "* express keywords as lemmas to ensure more effective matching \n",
    "* translate selected keywords to Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_file =\"../data/MdV_selectedCEOECB.csv\"\n",
    "mention_df = pd.read_csv(mention_file,sep=';',encoding = \"ISO-8859-1\")\n",
    "\n",
    "mention_df['wordnet_lemma']=mention_df.apply(lambda x:create_lemma(x['Mention']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB: As automatic translation is not stable or does not provide sufficient results, please use following workaround***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_file = \"../data/Translated_lemmas.csv\"\n",
    "translation_df = pd.read_csv(translation_file,sep=',',encoding = \"ISO-8859-1\") \n",
    "\n",
    "mention_df['Dutch']= translation_df['Dutch']\n",
    "# Add quotes in case of multiple words to enable 'phrase queries'.\n",
    "mention_df['Dutch']= mention_df.apply(lambda x:quote_phrase(x['Dutch']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dictionary with CEO class per unique keyword**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_mention_df = pd.DataFrame(data =mention_df[['CEO class','Dutch']]).drop_duplicates(subset= 'Dutch')\n",
    "ceo_mention_df2 = ceo_mention_df.drop(columns=['Dutch']).set_index(ceo_mention_df['Dutch'])\n",
    "ceo_mention_dict = ceo_mention_df2.transpose().to_dict(orient = 'records')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Construct query**\n",
    "\n",
    "* Sort keywords\n",
    "* Add double quotes to phrase queries\n",
    "* Concatenate all keywords into one query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_mention_list = list(ceo_mention_df['Dutch'])\n",
    "nl_mention_query = \",\".join(nl_mention_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Perform search**\n",
    "\n",
    "Using whoosh library:\n",
    "\n",
    "* Define query parser: which schema, which search fields, AND/OR search\n",
    "* Define searcher: which scoring approach\n",
    "* Store info from results object in pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from whoosh import scoring\n",
    "from whoosh import qparser\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "ix = open_dir(indexdir)\n",
    "\n",
    "parser = qparser.QueryParser(\"content\", schema=ix.schema,group=qparser.OrGroup)\n",
    "my_query = parser.parse(nl_mention_query)\n",
    "\n",
    "cols_list = []\n",
    "titles_list = []\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    results = searcher.search(my_query,limit=None, terms = True)\n",
    "    for res in results:\n",
    "        titles_list.append(res[\"title\"])\n",
    "        #row_dict = {}\n",
    "        col_dict = defaultdict(int)\n",
    "        hits = [term.decode('utf8')  for where,term in res.matched_terms()]\n",
    "        for hit in hits:\n",
    "            col_dict[hit]+= 1\n",
    "            #row_dict[hit] = row_dict.get(hit, 0) + 1  \n",
    "        cols_list.append(col_dict)\n",
    "\n",
    "#Create a dataframe for results of this search\n",
    "results_df = pd.DataFrame(cols_list)\n",
    "results_df.set_index([titles_list], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search results dataframe contains:\n",
    "* only those keywords that are found in documents\n",
    "* only those documents that have one or more keywords\n",
    "\n",
    "Combined dataframe contains: \n",
    "* all keywords, also those that are not present in documents\n",
    "* all documents, also those that have no keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe for all docs and keywords with empty values\n",
    "keywords_dic = {term:0 for term in nl_mention_list}\n",
    "list_docs = [doc['title'] for doc in ix.searcher().documents()] \n",
    "\n",
    "all_df = pd.DataFrame(keywords_dic, index = list_docs)\n",
    "\n",
    "# Create a dataframe for all docs and keywords with search results\n",
    "merged_df = results_df.combine_first(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently phrase queries are still broken up in separate search terms\n",
    "# this is shown by the surplus in columns in merged_df\n",
    "surplus = [col for col in merged_df if col not in all_df]\n",
    "# Remove these for now as a workaround; phrase queries should be fixed\n",
    "merged_df = merged_df.drop(columns = surplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Process and describe results** \n",
    "\n",
    "* Describe general characteristics of baseline search\n",
    "    * Original corpus size\n",
    "    * Number of keywords\n",
    "    * csv with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = ix.searcher().documents() \n",
    "summed_docs = sum(1 for x in all_docs)\n",
    "\n",
    "summed_results =len(results_df.index)\n",
    "percent_hits = (summed_results/summed_docs)*100\n",
    "\n",
    "percent_keywords = (len(results_df.columns)/len(nl_mention_list))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_summary = os.path.join(report,\"Baseline_summary.txt\")\n",
    "\n",
    "with open(report_summary, 'w') as file_handler:\n",
    "    # Add path to corpus\n",
    "    file_handler.write(\"Original corpus size: %s \\n\"%summed_docs)\n",
    "    file_handler.write(\"Number of snippets with keyword(s) present: %s \\n\"%summed_results)\n",
    "    file_handler.write(\"Percentage snippets with keywords(s) in corpus: %s \\n\"%percent_hits)\n",
    "    # Add path to keywords\n",
    "    file_handler.write(\"Total number of unique keywords found: %s \\n\"%len(results_df.columns))\n",
    "    file_handler.write(\"Percentage keywords found wrt to set used in query: %s \\n\"%percent_keywords)\n",
    "    file_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw data\n",
    "report_raw_data = os.path.join(report,\"Baseline_results.csv\")\n",
    "merged_df.to_csv(report_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze results\n",
    "    * nr keywords found per document\n",
    "    * nr keywords found per category\n",
    "    * nr hits found per keyword \n",
    "    * missed keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for documents\n",
    "sum_docs = pd.Series(merged_df.sum(axis=1)).value_counts(sort=True)\n",
    "sum_docs = sum_docs.sort_index()\n",
    "\n",
    "p1 = sum_docs.plot(kind='bar')\n",
    "\n",
    "p1.set_title('Figure 1: Distribution of keywords (total nr) in the searched corpus')\n",
    "\n",
    "p1.get_figure().savefig(os.path.join(report,\"Hits_per_document.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Series for keywords\n",
    "sum_keywords = pd.Series(merged_df.sum().iloc[:-1])\n",
    "\n",
    "sum_keywords = sum_keywords.sort_values(ascending=True)\n",
    "p2 = sum_keywords.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p2.set_title('Figure 2: Total nr of hits for top 35 most frequent keywords')\n",
    "\n",
    "p2.get_figure().savefig(os.path.join(report,\"Freq_keywords.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store list of keywords that were not present in analyzed corpus\n",
    "report_missed_keywords = os.path.join(report,\"Keywords_not_found_in_corpus.csv\")\n",
    "\n",
    "missed_keywords = sum_keywords[sum_keywords == 0]\n",
    "missed_keywords.to_csv(report_missed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ceo = pd.Series(ceo_mention_df2['CEO class']).value_counts(sort=True,ascending=True).drop(labels=['/B','/M','/P','\\P','/O'])\n",
    "\n",
    "p3 = count_ceo.plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p3.set_title('Figure 3: Distribution of classes of keywords in the query')\n",
    "\n",
    "p3.get_figure().savefig(os.path.join(report,\"Keyword_classes.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceo_sum_keywords = [ceo_mention_dict[i] for i in sum_keywords.index]\n",
    "\n",
    "serie = pd.Series(sum_keywords.values, index=ceo_sum_keywords)\n",
    "\n",
    "res = serie.groupby(serie.index).sum()\n",
    "res = res.drop(labels=['\\P','/M','/B']).sort_values()\n",
    "\n",
    "p4 = res.iloc[-35:-1].plot(kind='barh', figsize = (12,10))\n",
    "\n",
    "p4.set_title('Figure 4: Distribution of classes of keywords in search results')\n",
    "\n",
    "p4.get_figure().savefig(os.path.join(report,\"Freq_keywords_classes.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with groundtruth:\n",
    "* read in ground truth in dataframe\n",
    "* determine violent fragments (total nr and which fragments) NB correct for duplicate fragments\n",
    "* determine which fragments from baseline were correctly marked as violent\n",
    "* determine recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline search engine for EviDENce\n",
    "\n",
    "Search strategy\n",
    "\n",
    "1. Collect corpus to perform search on\n",
    "2. Index documents in corpus\n",
    "3. Collect Keywords\n",
    "4. Construct query\n",
    "5. Perform search\n",
    "6. Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Collect corpus to perform search on**\n",
    "\n",
    "Our corpus consists of oral history accounts.\n",
    "These are broken up in text fragments of 100 lemmas and can be found in a zip folder on surfdrive:\n",
    "\n",
    "../Data/NR-teksts/EviDENce_NR_output/TargetSize100/Lemma_preserve_paragraph.zip\n",
    "\n",
    "*Make sure to extract the zip folder on high-level location on your computer a to avoid \"path-too-long\" error*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide path to extracted folder with lemma fragments\n",
    "root = os.path.join(os.sep,\"media\",\"sf_MartinedeVos\")\n",
    "search_dir = os.path.join(os.sep,root,\"lem_par_150\",\"lemma_preserve_paragraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to alternative folder with lemma fragments\n",
    "surf = os.path.join(os.sep,root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Data\",\"NR-Teksts\",\"EviDENce_NR_output\")\n",
    "alt_search_dir = os.path.join(os.sep,surf,\"Size200\",\"fragmented_lemmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index all documents (i.e., lemma fragments) in the directory**\n",
    "\n",
    "* Create Schema\n",
    "* Add documents\n",
    "* Perform indexing\n",
    "\n",
    "_NB: this step only has to be run once, or when data is added or changed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_search import create_searchable_data\n",
    "\n",
    "# The creation of an index is only needed once; after that, opending the existing index is sufficient\n",
    "# in that case, the following line should be commented out\n",
    "\n",
    "#create_searchable_data(search_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Collect list of keywords from CEO-ECB mappings**\n",
    "\n",
    "Keywords are based on mappings from classes of the Circumstantial Event Ontology (CEO) on the ECB+ corpus\n",
    "\n",
    "Preprocessing entails:\n",
    "* express keywords as lemmas to ensure more effective matching \n",
    "* manually select keywords related to violence\n",
    "* translate selected keywords to Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions use the google translate API \n",
    "# As this API has stability issues, there is a workaround in the next cell\n",
    "from baseline_search import create_lemma_list\n",
    "from baseline_search import eng_to_dutch_list\n",
    "\n",
    "mention_file =\"../data/MdV_selectedCEOECB.csv\"\n",
    "\n",
    "en_mentions = create_lemma_list(mention_file)\n",
    "nl_mentions = eng_to_dutch_list(en_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prefab_file = \"../data/nl_mentions.csv\"\n",
    "prefab_mentions = pd.read_csv(prefab_file,sep=';',encoding = \"ISO-8859-1\")\n",
    "nl_mentions = list(prefab_mentions[\"Mention\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Construct query**\n",
    "\n",
    "* Sort keywords\n",
    "* Add double quotes to phrase queries\n",
    "* Concatenate all keywords into one query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_search import quote_phrases\n",
    "\n",
    "nl_mention_list = list(set(nl_mentions))\n",
    "quoted_nl_mention_list = quote_phrases (nl_mention_list)\n",
    "nl_mention_query = \",\".join(quoted_nl_mention_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Perform search**\n",
    "\n",
    "Using whoosh library:\n",
    "\n",
    "* Define query parser: which schema, which search fields, AND/OR search\n",
    "* Define searcher: which scoring approach\n",
    "* Store info from results object in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from whoosh import scoring\n",
    "from whoosh import qparser\n",
    "from whoosh.index import open_dir\n",
    "\n",
    "indexdir = os.path.join(os.sep,search_dir,\"indexdir\")\n",
    "ix = open_dir(indexdir)\n",
    "\n",
    "parser = qparser.QueryParser(\"content\", schema=ix.schema,group=qparser.OrGroup)\n",
    "my_query = parser.parse(nl_mention_query)\n",
    "\n",
    "cols_list = []\n",
    "titles_list = []\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    results = searcher.search(my_query,limit=None, terms = True)\n",
    "    for res in results:\n",
    "        titles_list.append(res[\"title\"])\n",
    "        #row_dict = {}\n",
    "        col_dict = defaultdict(int)\n",
    "        hits = [term.decode('utf8')  for where,term in res.matched_terms()]\n",
    "        for hit in hits:\n",
    "            col_dict[hit]+= 1\n",
    "            #row_dict[hit] = row_dict.get(hit, 0) + 1  \n",
    "        cols_list.append(col_dict)\n",
    "\n",
    "#Create a dataframe for results of this search, i.e. with a limited set of keywords \n",
    "results_df = pd.DataFrame(cols_list)\n",
    "results_df.set_index([titles_list], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete dataframe with: \n",
    "* columns for all search terms, also those that are not present in fragments\n",
    "* total column for total nr of hits per fragment\n",
    "* total row for total nr of hits per keyword\n",
    "* replace al non-numbers with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['\"Plat branden\"', '\"Rust in vrede\"', '\"dodelijk schieten\"',\n",
       "       '\"dood gaan\"', '\"dood schieten\"', '\"gegijzeld houden\"',\n",
       "       '\"gevangen nemen\"', '\"gevangenis periode\"', '\"grijp wapen\"',\n",
       "       '\"het schieten\"',\n",
       "       ...\n",
       "       'vlam', 'vlogen', 'vluchten', 'wapen', 'wond', 'worstelen', 'zeeroof',\n",
       "       'zelfmoord', 'zelfmoordpoging', 'zelfverdediging'],\n",
       "      dtype='object', length=188)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe for all docs and keywords\n",
    "keywords_dic = {term:0 for term in quoted_nl_mention_list}\n",
    "list_docs = [doc['title'] for doc in ix.searcher().documents()] \n",
    "\n",
    "all_df = pd.DataFrame(keywords_dic, index = list_docs)\n",
    "\n",
    "# Apparently phrase queries are still broken up in separate search terms\n",
    "# this is shown by the surplus in columns in merged df\n",
    "merged_df = results_df.combine_first(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aanval', 'alarm', 'arresteren', 'bedreiging', 'bedwelmen',\n",
       "       'behandelen', 'beschadigd', 'beschadigen', 'bezetten', 'bezetting',\n",
       "       ...\n",
       "       'verraden', 'verwoesten', 'verwoesting', 'verwonden', 'vlam',\n",
       "       'vluchten', 'wapen', 'wond', 'worstelen', 'zelfmoord'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = {key:0 for key in quoted_nl_mention_list}\n",
    "missed_terms1 = [term for term in all_terms.keys() if not term in results_df]\n",
    "found_terms = {key: value for d in cols_list for key, value in d.items()}\n",
    "\n",
    "new_columns = results_df.columns.tolist()+ missed_terms1\n",
    "results_df = results_df.reindex(columns = new_columns)\n",
    "\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "df['sum_docs'] = df.iloc[:,1:-1].sum(axis=1)\n",
    "df.loc['sum_keywords'] = df.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Process and describe results** \n",
    "\n",
    "* Describe general characteristics of baseline search\n",
    "    * Original corpus size\n",
    "    * Number of keywords\n",
    "* Add row and column to dataframe containing the summed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = ix.searcher().documents() \n",
    "sum_docs = sum(1 for x in all_docs)\n",
    "\n",
    "sum_results =len(df['title'])\n",
    "percent_hits = (sum_results/sum_docs)*100\n",
    "\n",
    "sum_keywords = sum(1 for x in nl_mention_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = 'Baseline results.txt'\n",
    "\n",
    "with open(report, 'w') as file_handler:\n",
    "    # Add path to corpus\n",
    "    file_handler.write(\"Original corpus size: %s \\n\"%sum_docs)\n",
    "    file_handler.write(\"Number of snippets with keyword(s) present: %s \\n\"%sum_results)\n",
    "    file_handler.write(\"Percentage snippets with keywords(s) in corpus: %s \\n\"%percent_hits)\n",
    "    # Add path to keywords\n",
    "    file_handler.write(\"Total number of keywords found: %s \\n\"%sum_keywords)\n",
    "    file_handler.write(\"Total number of unique keywords found: %s \\n\"%len(found_terms.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyze results\n",
    "    * nr keywords found per document\n",
    "    * nr keywords found per category\n",
    "    * nr hits found per keyword \n",
    "    * missed keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for keywords\n",
    "df_sumkw = pd.DataFrame(df.loc['sum_keywords'].iloc[1:-1])\n",
    "df_freqkw = df_sumkw.sort_values(by=['sum_keywords'],ascending=False).iloc[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot ten most frequent keywords\n",
    "keywords = df_freqkw.index.tolist()\n",
    "freqs = df_freqkw['sum_keywords'].tolist()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(freqs, labels=keywords, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.savefig(os.path.join(root,\"surfdrive\",\"Projects\", \"EviDENce\",\"Freq_keywords.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:**\n",
    "* Which keywords are not found?\n",
    "* How is the distribution of violence over the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_terms2 = df_sumkw.loc[df_sumkw['sum_keywords']==0].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.iloc[:,[0,-1]]\n",
    "all_docs = {x['title']:0 for x in ix.searcher().documents()}\n",
    "\n",
    "# Write in file: documents without keywords\n",
    "no_hit_docs = [item for item in list(all_docs.keys()) if item not in titles_list] \n",
    "\n",
    "with open('no_hits_snippets.txt', 'w') as file_handler:\n",
    "    for item in no_hit_docs:\n",
    "        file_handler.write(\"{}\\n\".format(item))\n",
    "        \n",
    "#df3 = df2.set_index('title')\n",
    "#df3\n",
    "#df3 = pd.DataFrame.from_dict(all_docs, orient ='index',columns =['sum_docs'])\n",
    "                             \n",
    "#res = pd.concat([df2,df3],sort=False)       \n",
    "\n",
    "#res\n",
    "#df3.columns = ['sum_docs']\n",
    "\n",
    "#print(df2.groupby(['sum_docs']).groups)\n",
    "#df2.value_counts().plot(kind='bar')\n",
    "\n",
    "#df2['sum_docs'].value_counts().plot(kind='bar')\n",
    "#df2.groupby(['sum_docs'])['title'].value_counts().plot(kind='bar')\n",
    "#df2.hist(column=['title'],by=['sum_docs'],bins=10,histtype='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
